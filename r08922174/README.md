# DMLS2020-FinalProj
### R08922174

## Centralized training
### Setups
Following Cheng-han's setup:
1. Apply Weighted Sampler
2. Standarize data before training

### How to run
One can simply change ```--train_path``` and ```--validation_path``` in ArgumentParser.

Run something such as: python3 local_full_train.py [personal args (optional)]

### Cautions
1. One should not set learning rate as large as 0.01, my default value is 0.001.
2. Centralized training, my training dataset comes from combining four siteX image floders, and the validation dataset remains the same as folder generated by sep_data.py.
3. How to obtain such training & validation image folders? 
    simply run my combine_site_data.sh after checking path's correctness and running Frank's sep_data.py!
4. Don't bother to specify arguments, since default values should work fine.

### Result
#### Downscaled Dataset size
Dataset size: 1000+
> ```[Updated 1/10]``` For downscaled results, see ```record_allsite.txt``` for reference.
#### Full Dataset size
Dataset size: 35289
Epoch   | Training Accuracy | Validation Accuracy
--------|-------------------|--------------------
1       |      96.38%       |        95.36%
2       |      96.72%       |        95.00%
3       |      98.80%       |        97.83%
4       |      96.80%       |        94.54%
5       |      99.54%       |        99.21%
6       |      99.44%       |        99.18%
7       |      99.35%       |        98.70%
8       |      99.69%       |        99.29%
9       |      99.46%       |        99.06%
10      |      99.05%       |        98.39%

## Distributed training
### Setups
#### Docker side
1. Get ubuntu:cryptenv7.tar from Frank.
2. ```docker load < ubuntu:cryptenv7.tar```
3. ```docker run -t -d -v /home/{PATH}/DMLS2020-FinalProj/:/FinalProj --gpus all --name crypten-container00{docker_index} ubuntu:cryptenv7```
4. ```docker exec -it crypten-container00{docker_index} /bin/bash```
5. ```cd /FinalProj/your folder```
#### Pytorch distributed side
1. ```python3 -m torch.distributed.launch --nproc_per_node={# of gpu available in each node(your machine)} --nnodes={# of workers you need} --node_rank={node index in current worker} --master_addr="{docker address}" --master_port=1234 distri_train.py --local_rank 0```
2. How to get docker address? 
    A: ```docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' {container_name_or_id}```
3. In **distri_train.py**, ```--local_rank``` argument meaning. This var should be modified once we wish to specify gpu_id to execute.(ex. two gpus in 140.112.90.52, two workers train simultaneously.) So this var should be set to 1 given single gpu available.

### How to run
For example, we spawn four workers to train single model.
1. Make 4 dockers available on your machine.
2. Enter 4 dockers and run ```python3 -m torch......--node_rank=#worker_id......--local_rank``` 0 respectively
3. Should be aware of ```--node_rank```; all the rest remains intact.
4. Wait and see result.

### Cautions
1. One should not set learning rate as large as 0.01, my default value is 0.001.
2. Training accuracy will be small, which owes to training dataset has been divided by ```# of workers``` to do distributed training, hence we should sum it up across all workers.

### Result
#### Downscaled Dataset size
Dataset size: 1000+
> ```[Updated 1/10]``` For downscaled results, see ```worker_0-3.txt``` for reference. (Train Acc should also be summed up by 4 workers.)
#### Full Dataset size
Dataset size: 35289
For example, 4 workers training scene.
#### Worker 1:![](https://i.imgur.com/saR4VZg.png)
#### Worker 2:![](https://i.imgur.com/zfrQkkt.png)
#### Worker 3:![](https://i.imgur.com/Q0yPTde.png)
#### Worker 4:![](https://i.imgur.com/KC7RQez.png)
#### Time consumed:
![](https://i.imgur.com/ZoEyFw0.png)
